Verification and validation of autonomous driving (AD)
systems and components is of increasing importance, as such
technology increases in real-world prevalence.
Safety-critical scenario generation is a key approach to
robustify AD policies through closed-loop training.
However, existing approaches for scenario generation rely
on simplistic objectives, resulting in overly-aggressive or
non-reactive adversarial behaviors. To generate diverse
adversarial yet realistic scenarios, we propose SEAL, a
scenario perturbation approach which leverages learned
scoring functions and adversarial, human-like skills.
SEAL-perturbed scenarios are more realistic than SOTA
baselines, leading to improved ego task success across
real-world, in-distribution, and out-of-distribution
scenarios, of more than 20%.

Code and instructions to be provided shortly.
